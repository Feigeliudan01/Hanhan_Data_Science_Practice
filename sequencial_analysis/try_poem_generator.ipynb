{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Poem Generator\n",
    "\n",
    "* I'm trying to generate poem\n",
    "* Method 1 - Character based sequence generation with LSTM\n",
    "* Method 2 - Word based sequence generation with LSTM\n",
    "* Method 3 - Character based sequence generation with CPT\n",
    "\n",
    "\n",
    "* Download the sonnets text from : https://github.com/pranjal52/text_generators/blob/master/sonnets.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import RNN\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ght never die,\\n but as'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_poem = open('sample_sonnets.txt').read().lower()\n",
    "sample_poem[77:99]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 1 - Character Based Poem Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":\n",
      "?\n"
     ]
    }
   ],
   "source": [
    "characters = sorted(list(set(sample_poem)))\n",
    "\n",
    "n_to_char = {n:char for n, char in enumerate(characters)}  # store characters and their index\n",
    "char_to_n = {char:n for n, char in enumerate(characters)}\n",
    "\n",
    "print(n_to_char[7])\n",
    "print(n_to_char[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "total_len = len(sample_poem)\n",
    "seq_len = 100  # each time we choose 100 character as a sequence and predict the next character after the sequence\n",
    "\n",
    "for i in range(total_len - seq_len):\n",
    "    seq = sample_poem[i:i+seq_len]\n",
    "    label = sample_poem[i+seq_len]\n",
    "    \n",
    "    X.append([char_to_n[char] for char in seq])\n",
    "    y.append(char_to_n[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM acceptable format, (number of sequneces(batch size), sequnece length (timesteps), number of features)\n",
    "X_modified = np.reshape(X, (len(X), seq_len, 1))  \n",
    "X_modified = X_modified / float(len(characters))  # normalize the value\n",
    "\n",
    "y_modified = np_utils.to_categorical(y)  # convert to one-hot format, there are 36 distinct characters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4325, 100, 1)\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      "  0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_modified.shape)\n",
    "print(y_modified[4:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # dropout is used for regularization\n",
    "model.add(LSTM(700, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(700))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4325/4325 [==============================] - 623s 144ms/step - loss: 3.0395\n",
      "Epoch 2/10\n",
      "4325/4325 [==============================] - 759s 175ms/step - loss: 3.0013\n",
      "Epoch 3/10\n",
      "4325/4325 [==============================] - 771s 178ms/step - loss: 3.0015\n",
      "Epoch 4/10\n",
      "4325/4325 [==============================] - 758s 175ms/step - loss: 2.9933\n",
      "Epoch 5/10\n",
      "4325/4325 [==============================] - 788s 182ms/step - loss: 2.9917\n",
      "Epoch 6/10\n",
      "4325/4325 [==============================] - 701s 162ms/step - loss: 2.9900\n",
      "Epoch 7/10\n",
      "4325/4325 [==============================] - 704s 163ms/step - loss: 2.9953\n",
      "Epoch 8/10\n",
      "4325/4325 [==============================] - 699s 162ms/step - loss: 2.9869\n",
      "Epoch 9/10\n",
      "4325/4325 [==============================] - 709s 164ms/step - loss: 2.9787\n",
      "Epoch 10/10\n",
      "4325/4325 [==============================] - 717s 166ms/step - loss: 2.9623\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29064b4dda0>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_modified, y_modified, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('poem_generator_gigantic.h5')  # save weights, so that later we can use without re-running the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('poem_generator_gigantic.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_poem_lst = []\n",
    "\n",
    "for j in range(77, 99):  # randomly choose some records and predict the sequence (generate the poem)\n",
    "    string_mapped = X[j]  \n",
    "    full_string = [n_to_char[value] for value in string_mapped]\n",
    "\n",
    "    for i in range(10):  # predict the next 10 character\n",
    "        x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "        x = x / float(len(characters))\n",
    "\n",
    "        # predict the next character\n",
    "        pred_index = np.argmax(model.predict(x, verbose=0))  \n",
    "        seq = [n_to_char[value] for value in string_mapped]\n",
    "        full_string.append(n_to_char[pred_index])\n",
    "\n",
    "        # predicted character will be added to support the next prediction\n",
    "        string_mapped.append(pred_index)\n",
    "        string_mapped = string_mapped[1:len(string_mapped)]\n",
    "        \n",
    "    new_poem_lst.extend(full_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ght never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " bu    t    tht never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but    t    tt never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but    t    t  never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but t      t   never die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but th      t   ever die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but tho      t   ver die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou       t  er die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou,      t   r die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou,      t   t die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, c     t    die,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, co     t    ie,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, con    t    te,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, cont      t   ,\n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contr       t  \n",
      " but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contra       t   but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contrac      t   but as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contract      t   ut as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracte      t   t as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted      t    as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted      t    as the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted t     t    s the riper should by time decease,\n",
      " his tender heir might bear his memory:\n",
      " but thou, contracted to     t    \n"
     ]
    }
   ],
   "source": [
    "generated_poem = ''.join(new_poem_lst)\n",
    "print(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation...\n",
    "I guess those readable words came from the original poem, they served as the testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 2 - Word Based Poem Generation\n",
    "\n",
    "### Simply map words to index, without tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age\n",
      "all\n"
     ]
    }
   ],
   "source": [
    "words = sorted(list(set(sample_poem.split())))\n",
    "\n",
    "n_to_word = {n:word for n, word in enumerate(words)}  # store characters and their index\n",
    "word_to_n = {word:n for n, word in enumerate(words)}\n",
    "\n",
    "print(n_to_word[7])\n",
    "print(n_to_word[9])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = []\n",
    "y = []\n",
    "all_words = sample_poem.split()\n",
    "total_len = len(all_words)\n",
    "seq_len = 100  # each time we choose 100 character as a sequence and predict the next character after the sequence\n",
    "\n",
    "for i in range(total_len - seq_len):\n",
    "    seq = all_words[i:i+seq_len]\n",
    "    label = all_words[i+seq_len]\n",
    "    \n",
    "    X.append([word_to_n[word] for word in seq])\n",
    "    y.append(word_to_n[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LSTM acceptable format, (number of sequneces(batch size), sequnece length (timesteps), number of features)\n",
    "X_modified = np.reshape(X, (len(X), seq_len, 1))  \n",
    "X_modified = X_modified / float(len(words))  # normalize the value\n",
    "\n",
    "y_modified = np_utils.to_categorical(y)  # convert to one-hot format, there are 36 distinct characters in total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(661, 100, 1)\n",
      "[[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "print(X_modified.shape)\n",
    "print(y_modified[4:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(700, input_shape=(X_modified.shape[1], X_modified.shape[2]), return_sequences=True))\n",
    "model.add(Dropout(0.2))  # dropout is used for regularization\n",
    "model.add(LSTM(700, return_sequences=True))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(LSTM(700))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Dense(y_modified.shape[1], activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "661/661 [==============================] - 91s 138ms/step - loss: 5.9895\n",
      "Epoch 2/10\n",
      "661/661 [==============================] - 83s 126ms/step - loss: 5.7600\n",
      "Epoch 3/10\n",
      "661/661 [==============================] - 84s 127ms/step - loss: 5.6341\n",
      "Epoch 4/10\n",
      "661/661 [==============================] - 84s 127ms/step - loss: 5.5812\n",
      "Epoch 5/10\n",
      "661/661 [==============================] - 87s 132ms/step - loss: 5.5664\n",
      "Epoch 6/10\n",
      "661/661 [==============================] - 90s 136ms/step - loss: 5.5553\n",
      "Epoch 7/10\n",
      "661/661 [==============================] - 86s 130ms/step - loss: 5.5377\n",
      "Epoch 8/10\n",
      "661/661 [==============================] - 97s 146ms/step - loss: 5.5548\n",
      "Epoch 9/10\n",
      "661/661 [==============================] - 93s 141ms/step - loss: 5.5511\n",
      "Epoch 10/10\n",
      "661/661 [==============================] - 93s 141ms/step - loss: 5.5504\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1950e4d1c18>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(X_modified, y_modified, epochs=10, batch_size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('poem_generator_gigantic_word.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights('poem_generator_gigantic_word.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_poem_lst = []\n",
    "\n",
    "for j in range(77, 99):  # randomly choose some records and predict the sequence (generate the poem)\n",
    "    string_mapped = X[j]  \n",
    "    full_string = []  # different from character based, here not recording the original sequence\n",
    "\n",
    "    for i in range(10):  # predict the next 10 character\n",
    "        x = np.reshape(string_mapped,(1,len(string_mapped), 1))\n",
    "        x = x / float(len(words))\n",
    "\n",
    "        # predict the next character\n",
    "        pred_index = np.argmax(model.predict(x, verbose=0))  \n",
    "        seq = [n_to_word[value] for value in string_mapped]\n",
    "        full_string.append(n_to_word[pred_index])\n",
    "\n",
    "        # predicted character will be added to support the next prediction\n",
    "        string_mapped.append(pred_index)\n",
    "        string_mapped = string_mapped[1:len(string_mapped)]\n",
    "        \n",
    "    new_poem_lst.extend(full_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou thou\n"
     ]
    }
   ],
   "source": [
    "generated_poem = ' '.join(new_poem_lst)\n",
    "print(generated_poem)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observation...\n",
    "\n",
    "* Word based method here tend to show too much words if we include original sequences, so here I removed them. Now you are seeing the real results, predicted sequences... Obviously, the number of epoch makes a difference.\n",
    "* The positive side here is, when using words, in fact there are less number of sequences used as training data, less meory. But also because of the small number of training data, it can be less accurate. Although in both cases here, none of them is accurate..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Method 3 - Character Based CPT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
