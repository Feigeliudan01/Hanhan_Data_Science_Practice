{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import os\n",
    "import keras\n",
    "import metrics\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import keras.backend as K\n",
    "\n",
    "from time import time\n",
    "\n",
    "from keras import callbacks\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD\n",
    "from keras.layers import Dense, Input\n",
    "from keras.initializers import VarianceScaling\n",
    "from keras.engine.topology import Layer, InputSpec\n",
    "\n",
    "from scipy.misc import imread\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score, normalized_mutual_info_score\n",
    "\n",
    "# run the code under DEC-keras\n",
    "## git clone https://github.com/XifengGuo/DEC-keras\n",
    "## cd DEC-keras\n",
    "\n",
    "# download the data from: https://datahack.analyticsvidhya.com/contest/practice-problem-identify-the-digits/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seed = 410  # I miss him, miss him so much\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = os.path.abspath('.')\n",
    "data_dir = os.path.join(root_dir, 'data', 'minist')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>filename</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.png</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.png</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2.png</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3.png</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4.png</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  filename  label\n",
       "0    0.png      4\n",
       "1    1.png      9\n",
       "2    2.png      1\n",
       "3    3.png      7\n",
       "4    4.png      3"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# I moved the downloaded data into /minist\n",
    "train = pd.read_csv(os.path.join(data_dir, 'train.csv'))\n",
    "test = pd.read_csv(os.path.join(data_dir, 'test.csv'))\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\niR9CiR9C/QC5SOw9zbV0vwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x122f2e5d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# randomly chose the digit to print out\n",
    "img_name = rng.choice(train.filename)\n",
    "filepath = os.path.join(data_dir, 'train', img_name)\n",
    "\n",
    "img = imread(filepath, flatten=True)\n",
    "\n",
    "pylab.imshow(img, cmap='gray')\n",
    "pylab.axis('off')\n",
    "pylab.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# store the images into numpy array\n",
    "def image2array(image_folder_path, filename_lst):\n",
    "    temp = []\n",
    "    for img_name in filename_lst.filename:\n",
    "        image_path = os.path.join(image_folder_path, img_name)\n",
    "        img = imread(image_path, flatten=True)\n",
    "        img = img.astype('float32')\n",
    "        temp.append(img)\n",
    "\n",
    "    data_x = np.stack(temp)\n",
    "\n",
    "    data_x /= 255.0\n",
    "    data_x = data_x.reshape(-1, 784).astype('float32')\n",
    "    \n",
    "    return data_x\n",
    "\n",
    "train_x = image2array(os.path.join(data_dir, 'train'), train)\n",
    "test_x = image2array(os.path.join(data_dir, 'test'), test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.],\n",
       "       [ 0.,  0.,  0., ...,  0.,  0.,  0.]], dtype=float32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_x[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 9, 4, 9, 3, 4])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_y = train.label.values\n",
    "train_y[4:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# divide the training data into training and validation\n",
    "split_size = int(train_x.shape[0]*0.7)\n",
    "\n",
    "train_x, val_x = train_x[:split_size], train_x[split_size:]\n",
    "train_y, val_y = train_y[:split_size], train_y[split_size:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KMeans(algorithm='auto', copy_x=True, init='k-means++', max_iter=300,\n",
       "    n_clusters=10, n_init=20, n_jobs=-1, precompute_distances='auto',\n",
       "    random_state=410, tol=0.0001, verbose=0)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model 1 - Just use kmeans\n",
    "km = KMeans(n_jobs=-1, n_clusters=10, n_init=20, random_state=410)  # n_init is the number of times to run\n",
    "km.fit(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49702986222381257"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = km.predict(val_x)\n",
    "normalized_mutual_info_score(val_y, pred)  # using normalized mutual info (NMI) for the evaluation, higher the better"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "dense_13 (Dense)             (None, 2000)              22000     \n",
      "_________________________________________________________________\n",
      "dense_14 (Dense)             (None, 500)               1000500   \n",
      "_________________________________________________________________\n",
      "dense_15 (Dense)             (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dense_16 (Dense)             (None, 784)               392784    \n",
      "=================================================================\n",
      "Total params: 3,330,794\n",
      "Trainable params: 3,330,794\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model 2 - autoencode to reduce dimension and extract useful info, then pass to kmeans\n",
    "## input placeholder\n",
    "input_img = Input(shape=(784,))\n",
    "\n",
    "## \"encoded\" is the encoded representation of the input\n",
    "encoded = Dense(500, activation='relu')(input_img)\n",
    "encoded = Dense(500, activation='relu')(encoded)\n",
    "encoded = Dense(2000, activation='relu')(encoded)\n",
    "encoded = Dense(10, activation='sigmoid')(encoded)\n",
    "\n",
    "## \"decoded\" is the lossy reconstruction of the input\n",
    "decoded = Dense(2000, activation='relu')(encoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(500, activation='relu')(decoded)\n",
    "decoded = Dense(784)(decoded)\n",
    "\n",
    "## this model maps an input to its reconstruction\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 34300 samples, validate on 14700 samples\n",
      "Epoch 1/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0341 - val_loss: 0.0273\n",
      "Epoch 2/30\n",
      "34300/34300 [==============================] - 25s - loss: 0.0255 - val_loss: 0.0244\n",
      "Epoch 3/30\n",
      "34300/34300 [==============================] - 29s - loss: 0.0236 - val_loss: 0.0234\n",
      "Epoch 4/30\n",
      "34300/34300 [==============================] - 30s - loss: 0.0229 - val_loss: 0.0231\n",
      "Epoch 5/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0226 - val_loss: 0.0228\n",
      "Epoch 6/30\n",
      "34300/34300 [==============================] - 26s - loss: 0.0224 - val_loss: 0.0226\n",
      "Epoch 7/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0221 - val_loss: 0.0224\n",
      "Epoch 8/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0219 - val_loss: 0.0222\n",
      "Epoch 9/30\n",
      "34300/34300 [==============================] - 25s - loss: 0.0217 - val_loss: 0.0220\n",
      "Epoch 10/30\n",
      "34300/34300 [==============================] - 27s - loss: 0.0214 - val_loss: 0.0217\n",
      "Epoch 11/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0212 - val_loss: 0.0215\n",
      "Epoch 12/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0210 - val_loss: 0.0214\n",
      "Epoch 13/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0208 - val_loss: 0.0212\n",
      "Epoch 14/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0207 - val_loss: 0.0213\n",
      "Epoch 15/30\n",
      "34300/34300 [==============================] - 21s - loss: 0.0205 - val_loss: 0.0208\n",
      "Epoch 16/30\n",
      "34300/34300 [==============================] - 23s - loss: 0.0202 - val_loss: 0.0207\n",
      "Epoch 17/30\n",
      "34300/34300 [==============================] - 25s - loss: 0.0201 - val_loss: 0.0205\n",
      "Epoch 18/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0201 - val_loss: 0.0205\n",
      "Epoch 19/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0198 - val_loss: 0.0202\n",
      "Epoch 20/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0196 - val_loss: 0.0201\n",
      "Epoch 21/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0196 - val_loss: 0.0202\n",
      "Epoch 22/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0194 - val_loss: 0.0199\n",
      "Epoch 23/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0195 - val_loss: 0.0199\n",
      "Epoch 24/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0191 - val_loss: 0.0197\n",
      "Epoch 25/30\n",
      "34300/34300 [==============================] - 22s - loss: 0.0189 - val_loss: 0.0195\n",
      "Epoch 26/30\n",
      "34300/34300 [==============================] - 21s - loss: 0.0188 - val_loss: 0.0195\n",
      "Epoch 27/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0187 - val_loss: 0.0194\n",
      "Epoch 28/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0186 - val_loss: 0.0194\n",
      "Epoch 29/30\n",
      "34300/34300 [==============================] - 23s - loss: 0.0187 - val_loss: 0.0195\n",
      "Epoch 30/30\n",
      "34300/34300 [==============================] - 26s - loss: 0.0185 - val_loss: 0.0192\n"
     ]
    }
   ],
   "source": [
    "# train the autoencoder model\n",
    "autoencoder.compile(optimizer='adam', loss='mse')\n",
    "train_history = autoencoder.fit(train_x, train_x, epochs=30, batch_size=2048, validation_data=(val_x, val_x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "##  this model maps an input to its encoded representation\n",
    "### this part takes a while\n",
    "encoder = Model(input_img, encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred_auto_train = encoder.predict(train_x)\n",
    "pred_auto = encoder.predict(val_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "km.fit(pred_auto_train)\n",
    "pred = km.predict(pred_auto)\n",
    "\n",
    "normalized_mutual_info_score(val_y, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# You need to copy the code from DEC-keras: https://github.com/XifengGuo/DEC-keras/blob/master/DEC.py\n",
    "\"\"\"\n",
    "Keras implementation for Deep Embedded Clustering (DEC) algorithm:\n",
    "\n",
    "Author:\n",
    "    Xifeng Guo. 2017.1.30\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    x = Input(shape=(dims[0],), name='input')\n",
    "    h = x\n",
    "\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        h = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(h)\n",
    "\n",
    "    # hidden layer\n",
    "    h = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(h)  # hidden layer, features are extracted from here\n",
    "\n",
    "    y = h\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        y = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(y)\n",
    "\n",
    "    # output\n",
    "    y = Dense(dims[0], kernel_initializer=init, name='decoder_0')(y)\n",
    "\n",
    "    return Model(inputs=x, outputs=y, name='AE'), Model(inputs=x, outputs=h, name='encoder')\n",
    "\n",
    "\n",
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight((self.n_clusters, input_dim), initializer='glorot_uniform', name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "                 q_ij = 1/(1+dist(x_i, u_j)^2), then normalize it.\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1))\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "\n",
    "class DEC(object):\n",
    "    def __init__(self,\n",
    "                 dims,\n",
    "                 n_clusters=10,\n",
    "                 alpha=1.0,\n",
    "                 init='glorot_uniform'):\n",
    "\n",
    "        super(DEC, self).__init__()\n",
    "\n",
    "        self.dims = dims\n",
    "        self.input_dim = dims[0]\n",
    "        self.n_stacks = len(self.dims) - 1\n",
    "\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.autoencoder, self.encoder = autoencoder(self.dims, init=init)\n",
    "\n",
    "        # prepare DEC model\n",
    "        clustering_layer = ClusteringLayer(self.n_clusters, name='clustering')(self.encoder.output)\n",
    "        self.model = Model(inputs=self.encoder.input, outputs=clustering_layer)\n",
    "\n",
    "    def pretrain(self, x, y=None, optimizer='adam', epochs=200, batch_size=256, save_dir='results/temp'):\n",
    "        print('...Pretraining...')\n",
    "        self.autoencoder.compile(optimizer=optimizer, loss='mse')\n",
    "\n",
    "        csv_logger = callbacks.CSVLogger(save_dir + '/pretrain_log.csv')\n",
    "        cb = [csv_logger]\n",
    "        if y is not None:\n",
    "            class PrintACC(callbacks.Callback):\n",
    "                def __init__(self, x, y):\n",
    "                    self.x = x\n",
    "                    self.y = y\n",
    "                    super(PrintACC, self).__init__()\n",
    "\n",
    "                def on_epoch_end(self, epoch, logs=None):\n",
    "                    if epoch % int(epochs/10) != 0:\n",
    "                        return\n",
    "                    feature_model = Model(self.model.input,\n",
    "                                          self.model.get_layer(\n",
    "                                              'encoder_%d' % (int(len(self.model.layers) / 2) - 1)).output)\n",
    "                    features = feature_model.predict(self.x)\n",
    "                    km = KMeans(n_clusters=len(np.unique(self.y)), n_init=20, n_jobs=4)\n",
    "                    y_pred = km.fit_predict(features)\n",
    "                    # print()\n",
    "                    print(' '*8 + '|==>  acc: %.4f,  nmi: %.4f  <==|'\n",
    "                          % (metrics.acc(self.y, y_pred), metrics.nmi(self.y, y_pred)))\n",
    "\n",
    "            cb.append(PrintACC(x, y))\n",
    "\n",
    "        # begin pretraining\n",
    "        t0 = time()\n",
    "        self.autoencoder.fit(x, x, batch_size=batch_size, epochs=epochs, callbacks=cb)\n",
    "        print('Pretraining time: ', time() - t0)\n",
    "        self.autoencoder.save_weights(save_dir + '/ae_weights.h5')\n",
    "        print('Pretrained weights are saved to %s/ae_weights.h5' % save_dir)\n",
    "        self.pretrained = True\n",
    "\n",
    "    def load_weights(self, weights):  # load weights of DEC model\n",
    "        self.model.load_weights(weights)\n",
    "\n",
    "    def extract_features(self, x):\n",
    "        return self.encoder.predict(x)\n",
    "\n",
    "    def predict(self, x):  # predict cluster labels using the output of clustering layer\n",
    "        q = self.model.predict(x, verbose=0)\n",
    "        return q.argmax(1)\n",
    "\n",
    "    @staticmethod\n",
    "    def target_distribution(q):\n",
    "        weight = q ** 2 / q.sum(0)\n",
    "        return (weight.T / weight.sum(1)).T\n",
    "\n",
    "    def compile(self, optimizer='sgd', loss='kld'):\n",
    "        self.model.compile(optimizer=optimizer, loss=loss)\n",
    "\n",
    "    def fit(self, x, y=None, maxiter=2e4, batch_size=256, tol=1e-3,\n",
    "            update_interval=140, save_dir='./results/temp'):\n",
    "\n",
    "        print('Update interval', update_interval)\n",
    "        save_interval = x.shape[0] / batch_size * 5  # 5 epochs\n",
    "        print('Save interval', save_interval)\n",
    "\n",
    "        # Step 1: initialize cluster centers using k-means\n",
    "        t1 = time()\n",
    "        print('Initializing cluster centers with k-means.')\n",
    "        kmeans = KMeans(n_clusters=self.n_clusters, n_init=20)\n",
    "        y_pred = kmeans.fit_predict(self.encoder.predict(x))\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        self.model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])\n",
    "\n",
    "        # Step 2: deep clustering\n",
    "        # logging file\n",
    "        import csv\n",
    "        logfile = open(save_dir + '/dec_log.csv', 'w')\n",
    "        logwriter = csv.DictWriter(logfile, fieldnames=['iter', 'acc', 'nmi', 'ari', 'loss'])\n",
    "        logwriter.writeheader()\n",
    "\n",
    "        loss = 0\n",
    "        index = 0\n",
    "        index_array = np.arange(x.shape[0])\n",
    "        for ite in range(int(maxiter)):\n",
    "            if ite % update_interval == 0:\n",
    "                q = self.model.predict(x, verbose=0)\n",
    "                p = self.target_distribution(q)  # update the auxiliary target distribution p\n",
    "\n",
    "                # evaluate the clustering performance\n",
    "                y_pred = q.argmax(1)\n",
    "                if y is not None:\n",
    "                    acc = np.round(metrics.acc(y, y_pred), 5)\n",
    "                    nmi = np.round(metrics.nmi(y, y_pred), 5)\n",
    "                    ari = np.round(metrics.ari(y, y_pred), 5)\n",
    "                    loss = np.round(loss, 5)\n",
    "                    logdict = dict(iter=ite, acc=acc, nmi=nmi, ari=ari, loss=loss)\n",
    "                    logwriter.writerow(logdict)\n",
    "                    print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f' % (ite, acc, nmi, ari), ' ; loss=', loss)\n",
    "\n",
    "                # check stop criterion\n",
    "                delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "                y_pred_last = np.copy(y_pred)\n",
    "                if ite > 0 and delta_label < tol:\n",
    "                    print('delta_label ', delta_label, '< tol ', tol)\n",
    "                    print('Reached tolerance threshold. Stopping training.')\n",
    "                    logfile.close()\n",
    "                    break\n",
    "\n",
    "            # train on batch\n",
    "            # if index == 0:\n",
    "            #     np.random.shuffle(index_array)\n",
    "            idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "            self.model.train_on_batch(x=x[idx], y=p[idx])\n",
    "            index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "\n",
    "            # save intermediate model\n",
    "            if ite % save_interval == 0:\n",
    "                print('saving model to:', save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "                self.model.save_weights(save_dir + '/DEC_model_' + str(ite) + '.h5')\n",
    "\n",
    "            ite += 1\n",
    "\n",
    "        # save the trained model\n",
    "        logfile.close()\n",
    "        print('saving model to:', save_dir + '/DEC_model_final.h5')\n",
    "        self.model.save_weights(save_dir + '/DEC_model_final.h5')\n",
    "\n",
    "        return y_pred\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...Pretraining...\n",
      "Epoch 1/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.1106        |==>  acc: 0.1818,  nmi: 0.1018  <==|\n",
      "34300/34300 [==============================] - 27s - loss: 0.1101    \n",
      "Epoch 2/30\n",
      "34300/34300 [==============================] - 20s - loss: 0.0723    \n",
      "Epoch 3/30\n",
      "34300/34300 [==============================] - 27s - loss: 0.0647    \n",
      "Epoch 4/30\n",
      "32768/34300 [===========================>..] - ETA: 1s - loss: 0.0638        |==>  acc: 0.1893,  nmi: 0.0992  <==|\n",
      "34300/34300 [==============================] - 32s - loss: 0.0638    \n",
      "Epoch 5/30\n",
      "34300/34300 [==============================] - 24s - loss: 0.0635    \n",
      "Epoch 6/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0633    \n",
      "Epoch 7/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0607        |==>  acc: 0.2809,  nmi: 0.2014  <==|\n",
      "34300/34300 [==============================] - 27s - loss: 0.0606    \n",
      "Epoch 8/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0568    \n",
      "Epoch 9/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0549    \n",
      "Epoch 10/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0514        |==>  acc: 0.3762,  nmi: 0.3369  <==|\n",
      "34300/34300 [==============================] - 26s - loss: 0.0513    \n",
      "Epoch 11/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0483    \n",
      "Epoch 12/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0459    \n",
      "Epoch 13/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0431        |==>  acc: 0.4497,  nmi: 0.4353  <==|\n",
      "34300/34300 [==============================] - 27s - loss: 0.0430    \n",
      "Epoch 14/30\n",
      "34300/34300 [==============================] - 19s - loss: 0.0394    \n",
      "Epoch 15/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0361    \n",
      "Epoch 16/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0338        |==>  acc: 0.5160,  nmi: 0.4866  <==|\n",
      "34300/34300 [==============================] - 26s - loss: 0.0337    \n",
      "Epoch 17/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0320    \n",
      "Epoch 18/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0309    \n",
      "Epoch 19/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0297        |==>  acc: 0.5186,  nmi: 0.4903  <==|\n",
      "34300/34300 [==============================] - 29s - loss: 0.0297    \n",
      "Epoch 20/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0288    \n",
      "Epoch 21/30\n",
      "34300/34300 [==============================] - 20s - loss: 0.0279    \n",
      "Epoch 22/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0268        |==>  acc: 0.5362,  nmi: 0.5069  <==|\n",
      "34300/34300 [==============================] - 27s - loss: 0.0268    \n",
      "Epoch 23/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0263    \n",
      "Epoch 24/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0256    \n",
      "Epoch 25/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0249        |==>  acc: 0.6018,  nmi: 0.5373  <==|\n",
      "34300/34300 [==============================] - 26s - loss: 0.0249    \n",
      "Epoch 26/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0243    \n",
      "Epoch 27/30\n",
      "34300/34300 [==============================] - 17s - loss: 0.0239    \n",
      "Epoch 28/30\n",
      "32768/34300 [===========================>..] - ETA: 0s - loss: 0.0233        |==>  acc: 0.6034,  nmi: 0.5373  <==|\n",
      "34300/34300 [==============================] - 26s - loss: 0.0233    \n",
      "Epoch 29/30\n",
      "34300/34300 [==============================] - 19s - loss: 0.0227    \n",
      "Epoch 30/30\n",
      "34300/34300 [==============================] - 18s - loss: 0.0223    \n",
      "('Pretraining time: ', 664.2266590595245)\n",
      "Pretrained weights are saved to DEC_results/ae_weights.h5\n"
     ]
    }
   ],
   "source": [
    "# Model 3 - DEC Keras\n",
    "\n",
    "# setting the hyper parameters\n",
    "init = 'glorot_uniform'\n",
    "pretrain_optimizer = 'adam'\n",
    "dataset = 'mnist'\n",
    "batch_size = 2048\n",
    "maxiter = 7\n",
    "tol = 0.001\n",
    "save_dir = 'DEC_results'\n",
    "\n",
    "import os\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)\n",
    "\n",
    "update_interval = 200\n",
    "pretrain_epochs = 30\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in',\n",
    "                       distribution='uniform')  \n",
    "\n",
    "\n",
    "# prepare the DEC model\n",
    "dec = DEC(dims=[train_x.shape[-1], 500, 500, 2000, 10], n_clusters=10, init=init)\n",
    "\n",
    "dec.pretrain(x=train_x, y=train_y, optimizer=pretrain_optimizer,\n",
    "             epochs=pretrain_epochs, batch_size=batch_size,\n",
    "             save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input (InputLayer)           (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "encoder_0 (Dense)            (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "encoder_1 (Dense)            (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "encoder_2 (Dense)            (None, 2000)              1002000   \n",
      "_________________________________________________________________\n",
      "encoder_3 (Dense)            (None, 10)                20010     \n",
      "_________________________________________________________________\n",
      "clustering (ClusteringLayer) (None, 10)                100       \n",
      "=================================================================\n",
      "Total params: 1,665,110\n",
      "Trainable params: 1,665,110\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "dec.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dec.compile(optimizer=SGD(0.01, 0.9), loss='kld')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Update interval', 200)\n",
      "('Save interval', 80)\n",
      "Initializing cluster centers with k-means.\n",
      "('Iter 0: acc = 0.61656, nmi = 0.55830, ari = 0.45864', ' ; loss=', 0)\n",
      "('saving model to:', 'DEC_results/DEC_model_0.h5')\n",
      "('saving model to:', 'DEC_results/DEC_model_final.h5')\n"
     ]
    }
   ],
   "source": [
    "y_pred = dec.fit(train_x, y=train_y, tol=tol, maxiter=maxiter, batch_size=batch_size,\n",
    "                 update_interval=update_interval, save_dir=save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.54670800216786986"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_val = dec.predict(val_x)\n",
    "normalized_mutual_info_score(val_y, pred_val)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
